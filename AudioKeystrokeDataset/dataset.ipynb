{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../data/Zoom/Raw Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(filepath):\n",
    "    label = os.path.basename(filepath)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 70 audio files.\n"
     ]
    }
   ],
   "source": [
    "def load_wav_files(dataset_path):\n",
    "    audio_data = []\n",
    "    labels = [] \n",
    "\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    signal, sr = librosa.load(file_path, sr=None)\n",
    "                    audio_data.append(signal)\n",
    "                    label = extract_label(file_path)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return audio_data, labels\n",
    "\n",
    "audio_data, labels = load_wav_files(DATASET_PATH)\n",
    "print(f\"Loaded {len(audio_data)} audio files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_keystrokes(audio, sr, threshold, segment_length=14400, n_fft=1024, hop_length=256):\n",
    "    \"\"\"\n",
    "    Isolate keystrokes from audio using Short-Time Fourier Transform (STFT).\n",
    "    Parameters:\n",
    "    - audio: The audio signal to process.\n",
    "    - sr: Sample rate of the audio.\n",
    "    - threshold: Energy threshold for detecting keystrokes.\n",
    "    - segment_length: Length of the segment to extract around each keystroke.\n",
    "    - n_fft: Number of FFT components.\n",
    "    - hop_length: Number of samples between frames.\n",
    "    Returns:\n",
    "    - segments: List of isolated keystroke segments.\n",
    "    \"\"\"\n",
    "    # Compute the Fast Fourier Transform\n",
    "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    # Sum the magnitude of the STFT across frequency bins to get energy\n",
    "    energy = np.sum(np.abs(stft), axis=0)\n",
    "    # Choose frames based on threashold\n",
    "    keystroke_frames = np.where(energy > threshold)[0]\n",
    "\n",
    "    # Group contiguous frames\n",
    "    groups = np.split(keystroke_frames, np.where(np.diff(keystroke_frames) > 1)[0] + 1)\n",
    "    \n",
    "    # Take keystroke as center. Extract segment centered on the keystroke\n",
    "    segments = []\n",
    "    for group in groups:\n",
    "        center_frame = int(np.mean(group))\n",
    "        start = max((center_frame * hop_length) - segment_length // 2, 0)\n",
    "        end = start + segment_length\n",
    "        if end > len(audio):\n",
    "            start = len(audio) - segment_length\n",
    "            end = len(audio)\n",
    "        segments.append(audio[start:end])\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_keystroke_threshold(audio, sr, initial_threshold=100, step=5, target_keystrokes=25):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1: Zoom Keystroke Threshold Setting.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio: 1D numpy array of the audio signal.\n",
    "    - sr: Sample rate.\n",
    "    - initial_threshold: Initial energy threshold.\n",
    "    - step: Step size for adjusting threshold.\n",
    "    - target_keystrokes: Desired number of keystrokes.\n",
    "    \n",
    "    Returns:\n",
    "    - Optimal threshold value.\n",
    "    \"\"\"\n",
    "    P = initial_threshold\n",
    "    iterations = 100\n",
    "    while True and iterations > 0:\n",
    "        iterations -= 1\n",
    "        S = isolate_keystrokes(audio, sr, threshold=P)\n",
    "        num_keystrokes = len(S)\n",
    "        if num_keystrokes == target_keystrokes:\n",
    "            break\n",
    "        elif num_keystrokes < target_keystrokes:\n",
    "            P -= step\n",
    "        elif num_keystrokes > target_keystrokes:\n",
    "            P += step\n",
    "        step *= 0.99 \n",
    "    return P, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift(segment, max_shift_percentage=0.4):\n",
    "    \"\"\"\n",
    "    Shift the audio segment in time by a random amount.\n",
    "    Parameters:\n",
    "    - segment: The audio segment to shift.\n",
    "    - max_shift_percentage: Maximum percentage of the segment length to shift.\n",
    "    Returns:\n",
    "    - shifted_segment: The time-shifted audio segment.\n",
    "    \"\"\"\n",
    "    # Calculate the maximum shift in samples\n",
    "    max_shift = int(len(segment) * max_shift_percentage)\n",
    "    # Take a random integer shift within the range [-max_shift, max_shift]\n",
    "    shift = np.random.randint(-max_shift, max_shift)\n",
    "    # Shift the segment\n",
    "    shifted_segment = np.roll(segment, shift)\n",
    "    # Zero out the wrapped portion to avoid artifacts\n",
    "    if shift > 0:\n",
    "        shifted_segment[:shift] = 0\n",
    "    elif shift < 0:\n",
    "        shifted_segment[shift:] = 0\n",
    "    return shifted_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mel_spectrogram(segment, sr, n_mels=64, n_fft=1024, hop_length=225):\n",
    "    \"\"\"\n",
    "    Generate a Mel spectrogram from the audio segment.\n",
    "    Parameters:\n",
    "    - segment: The audio segment to process.\n",
    "    - sr: Sample rate of the audio.\n",
    "    - n_mels: Number of Mel bands to generate.\n",
    "    - n_fft: Number of FFT components.\n",
    "    - hop_length: Number of samples between frames.\n",
    "    Returns:\n",
    "    - S_dB: The Mel spectrogram in decibels.\n",
    "    \"\"\"\n",
    "    S = librosa.feature.melspectrogram(y=segment, sr=sr, n_fft=n_fft,  hop_length=hop_length, n_mels=n_mels)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_augment(spectrogram, time_mask_percentage=0.1, freq_mask_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Augment the Mel spectrogram.\n",
    "    Parameters:\n",
    "    - spectrogram: The Mel spectrogram to augment.\n",
    "    - time_mask_percentage: Percentage of the time dimension to mask.\n",
    "    - freq_mask_percentage: Percentage of the frequency dimension to mask.\n",
    "    Returns:\n",
    "    - augmented_spec: The augmented Mel spectrogram.\n",
    "    \"\"\"\n",
    "    augmented_spec = spectrogram.copy()\n",
    "    n_mels, n_time = augmented_spec.shape\n",
    "    # Calculate the lengths of the masks\n",
    "    time_mask_length = max(1, int(n_time * time_mask_percentage))\n",
    "    freq_mask_length = max(1, int(n_mels * freq_mask_percentage))\n",
    "    # Randomly select the starting points for the masks\n",
    "    time_start = np.random.randint(0, n_time - time_mask_length + 1)\n",
    "    freq_start = np.random.randint(0, n_mels - freq_mask_length + 1)\n",
    "    # Calculate the mean value of the spectrogram to use as the mask value\n",
    "    mask_value = np.mean(augmented_spec)\n",
    "    # Apply the time mask and frequency mask\n",
    "    augmented_spec[freq_start:freq_start + freq_mask_length, time_start:time_start + time_mask_length] = mask_value\n",
    "    return augmented_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(file_path, energy_threshold, output_dir, segment_length=14400, sr=44100):\n",
    "    \"\"\"\n",
    "    Process a single audio file to isolate keystrokes and save the segments.\n",
    "    Parameters:\n",
    "    - file_path: Path to the audio file.\n",
    "    - energy_threshold: Energy threshold for isolating keystrokes.\n",
    "    - output_dir: Directory to save the processed segments.\n",
    "    - segment_length: Length of the segment to extract around each keystroke.\n",
    "    - sr: Sample rate for loading the audio.\n",
    "    Returns:\n",
    "    - sample_paths: List of paths to the saved audio segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=sr)\n",
    "    segments = isolate_keystrokes(audio, sr, threshold=energy_threshold, segment_length=segment_length)\n",
    "    \n",
    "    sample_paths = []\n",
    "    for idx, segment in enumerate(segments):\n",
    "        # Apply time shift for data augmentation\n",
    "        augmented_segment = time_shift(segment)\n",
    "        # Save the raw audio segment (can be used for further processing or as input to a model)\n",
    "        segment_path = os.path.join(output_dir, f\"keystroke_{idx}.wav\")\n",
    "        sf.write(segment_path, augmented_segment, sr)\n",
    "        sample_paths.append(segment_path)\n",
    "        \n",
    "        # Optionally, generate and save the mel-spectrogram image\n",
    "        mel_spec = generate_mel_spectrogram(augmented_segment, sr)\n",
    "        mel_spec_aug = spec_augment(mel_spec)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        librosa.display.specshow(mel_spec_aug, sr=sr, x_axis='time', y_axis='mel', cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        image_path = os.path.join(output_dir, f\"keystroke_{idx}.png\")\n",
    "        plt.savefig(image_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "    return sample_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioKeystrokeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioKeystrokeDataset(Dataset):\n",
    "    def __init__(self, dataset_path, energy_threshold=100, segment_length=14400,\n",
    "                 sr=44100, n_fft=1024, hop_length=256, n_mels=64, spec_hop_length=500,\n",
    "                 target_keystrokes=35):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          dataset_path: Path to the dataset directory containing key .wav files.\n",
    "          energy_threshold: Initial energy threshold for keystroke isolation.\n",
    "          segment_length: Number of samples per isolated keystroke.\n",
    "          sr: Sample rate for audio.\n",
    "          n_fft: FFT window size for STFT.\n",
    "          hop_length: Hop length for STFT.\n",
    "          n_mels: Number of mel bands for spectrogram.\n",
    "          spec_hop_length: Hop length used in mel spectrogram generation.\n",
    "          target_keystrokes: Expected number of keystrokes per file.\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.energy_threshold = energy_threshold\n",
    "        self.segment_length = segment_length\n",
    "        self.sr = sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.spec_hop_length = spec_hop_length\n",
    "        self.target_keystrokes = target_keystrokes\n",
    "        \n",
    "        # List to store (mel-spectrogram, label) tuples\n",
    "        self.samples = []\n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Walk through the dataset path, process each .wav file,\n",
    "        isolate keystrokes, and generate augmented mel-spectrograms.\n",
    "        \"\"\"\n",
    "        # Gather all .wav file paths first for tqdm progress bar.\n",
    "        all_files = []\n",
    "        for root, _, files in os.walk(self.dataset_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.wav'):\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "                    \n",
    "        # Process each file with tqdm for progress visualization.\n",
    "        for file_path in tqdm(all_files, desc=\"Processing Audio Files\"):\n",
    "            label = extract_label(file_path)\n",
    "            try:\n",
    "                audio, sr = librosa.load(file_path, sr=self.sr)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Compute optimal threshold and isolate keystrokes from the file.\n",
    "            optimal_threshold, segments = zoom_keystroke_threshold(\n",
    "                audio, sr, initial_threshold=self.energy_threshold,\n",
    "                target_keystrokes=self.target_keystrokes\n",
    "            )\n",
    "            \n",
    "            # Process each keystroke segment.\n",
    "            for segment in segments:\n",
    "                # Apply random time shift for data augmentation.\n",
    "                augmented_segment = time_shift(segment)\n",
    "                # Generate mel-spectrogram and apply SpecAugment.\n",
    "                mel_spec = generate_mel_spectrogram(\n",
    "                    augmented_segment, sr, n_mels=self.n_mels,\n",
    "                    n_fft=self.n_fft, hop_length=self.spec_hop_length\n",
    "                )\n",
    "                mel_spec_aug = spec_augment(mel_spec)\n",
    "                self.samples.append((mel_spec_aug, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mel_spec, label = self.samples[idx]\n",
    "        # Normalize the spectrogram to the range [0, 1]\n",
    "        mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min() + 1e-6)\n",
    "        # Optionally convert label to integer if keys are represented as numbers\n",
    "        try:\n",
    "            label = int(label)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return mel_spec, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Audio Files: 100%|██████████| 70/70 [00:25<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 2450 keystroke samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"../data/Zoom/Raw Data\"\n",
    "dataset = AudioKeystrokeDataset(DATASET_PATH)\n",
    "print(f\"Dataset contains {len(dataset)} keystroke samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1960\n",
      "Testing dataset size: 490\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Testing dataset size: {len(test_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
