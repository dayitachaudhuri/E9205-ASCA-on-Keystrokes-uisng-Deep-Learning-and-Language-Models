{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../data/Zoom/Raw Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(filepath):\n",
    "    label = os.path.basename(filepath)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 70 audio files.\n"
     ]
    }
   ],
   "source": [
    "def load_wav_files(dataset_path):\n",
    "    audio_data = []\n",
    "    labels = [] \n",
    "\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    signal, sr = librosa.load(file_path, sr=None)\n",
    "                    audio_data.append(signal)\n",
    "                    label = extract_label(file_path)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return audio_data, labels\n",
    "\n",
    "audio_data, labels = load_wav_files(DATASET_PATH)\n",
    "print(f\"Loaded {len(audio_data)} audio files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_keystrokes(audio, sr, threshold, segment_length=14400, n_fft=1024, hop_length=256):\n",
    "    \"\"\"\n",
    "    Isolate keystrokes from audio using Short-Time Fourier Transform (STFT).\n",
    "    Parameters:\n",
    "    - audio: The audio signal to process.\n",
    "    - sr: Sample rate of the audio.\n",
    "    - threshold: Energy threshold for detecting keystrokes.\n",
    "    - segment_length: Length of the segment to extract around each keystroke.\n",
    "    - n_fft: Number of FFT components.\n",
    "    - hop_length: Number of samples between frames.\n",
    "    Returns:\n",
    "    - segments: List of isolated keystroke segments.\n",
    "    \"\"\"\n",
    "    # Compute the Fast Fourier Transform\n",
    "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    # Sum the magnitude of the STFT across frequency bins to get energy\n",
    "    energy = np.sum(np.abs(stft), axis=0)\n",
    "    # Choose frames based on threashold\n",
    "    keystroke_frames = np.where(energy > threshold)[0]\n",
    "\n",
    "    # Group contiguous frames\n",
    "    groups = np.split(keystroke_frames, np.where(np.diff(keystroke_frames) > 1)[0] + 1)\n",
    "    \n",
    "    # Take keystroke as center. Extract segment centered on the keystroke\n",
    "    segments = []\n",
    "    for group in groups:\n",
    "        center_frame = int(np.mean(group))\n",
    "        start = max((center_frame * hop_length) - segment_length // 2, 0)\n",
    "        end = start + segment_length\n",
    "        if end > len(audio):\n",
    "            start = len(audio) - segment_length\n",
    "            end = len(audio)\n",
    "        segments.append(audio[start:end])\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_keystroke_threshold(audio, sr, initial_threshold=100, step=5, target_keystrokes=25):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1: Zoom Keystroke Threshold Setting.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio: 1D numpy array of the audio signal.\n",
    "    - sr: Sample rate.\n",
    "    - initial_threshold: Initial energy threshold.\n",
    "    - step: Step size for adjusting threshold.\n",
    "    - target_keystrokes: Desired number of keystrokes.\n",
    "    \n",
    "    Returns:\n",
    "    - Optimal threshold value.\n",
    "    \"\"\"\n",
    "    P = initial_threshold\n",
    "    while True:\n",
    "        S = isolate_keystrokes(audio, sr, threshold=P)\n",
    "        num_keystrokes = len(S)\n",
    "        if num_keystrokes == target_keystrokes:\n",
    "            break\n",
    "        elif num_keystrokes < target_keystrokes:\n",
    "            P -= step\n",
    "        elif num_keystrokes > target_keystrokes:\n",
    "            P += step\n",
    "        step *= 0.99 \n",
    "    return P, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift(segment, max_shift_percentage=0.4):\n",
    "    \"\"\"\n",
    "    Shift the audio segment in time by a random amount.\n",
    "    Parameters:\n",
    "    - segment: The audio segment to shift.\n",
    "    - max_shift_percentage: Maximum percentage of the segment length to shift.\n",
    "    Returns:\n",
    "    - shifted_segment: The time-shifted audio segment.\n",
    "    \"\"\"\n",
    "    # Calculate the maximum shift in samples\n",
    "    max_shift = int(len(segment) * max_shift_percentage)\n",
    "    # Take a random integer shift within the range [-max_shift, max_shift]\n",
    "    shift = np.random.randint(-max_shift, max_shift)\n",
    "    # Shift the segment\n",
    "    shifted_segment = np.roll(segment, shift)\n",
    "    # Zero out the wrapped portion to avoid artifacts\n",
    "    if shift > 0:\n",
    "        shifted_segment[:shift] = 0\n",
    "    elif shift < 0:\n",
    "        shifted_segment[shift:] = 0\n",
    "    return shifted_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mel_spectrogram(segment, sr, n_mels=64, n_fft=1024, hop_length=225):\n",
    "    \"\"\"\n",
    "    Generate a Mel spectrogram from the audio segment.\n",
    "    Parameters:\n",
    "    - segment: The audio segment to process.\n",
    "    - sr: Sample rate of the audio.\n",
    "    - n_mels: Number of Mel bands to generate.\n",
    "    - n_fft: Number of FFT components.\n",
    "    - hop_length: Number of samples between frames.\n",
    "    Returns:\n",
    "    - S_dB: The Mel spectrogram in decibels.\n",
    "    \"\"\"\n",
    "    S = librosa.feature.melspectrogram(y=segment, sr=sr, n_fft=n_fft,  hop_length=hop_length, n_mels=n_mels)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_dB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_augment(spectrogram, time_mask_percentage=0.1, freq_mask_percentage=0.1):\n",
    "    \"\"\"\n",
    "    Augment the Mel spectrogram.\n",
    "    Parameters:\n",
    "    - spectrogram: The Mel spectrogram to augment.\n",
    "    - time_mask_percentage: Percentage of the time dimension to mask.\n",
    "    - freq_mask_percentage: Percentage of the frequency dimension to mask.\n",
    "    Returns:\n",
    "    - augmented_spec: The augmented Mel spectrogram.\n",
    "    \"\"\"\n",
    "    augmented_spec = spectrogram.copy()\n",
    "    n_mels, n_time = augmented_spec.shape\n",
    "    # Calculate the lengths of the masks\n",
    "    time_mask_length = max(1, int(n_time * time_mask_percentage))\n",
    "    freq_mask_length = max(1, int(n_mels * freq_mask_percentage))\n",
    "    # Randomly select the starting points for the masks\n",
    "    time_start = np.random.randint(0, n_time - time_mask_length + 1)\n",
    "    freq_start = np.random.randint(0, n_mels - freq_mask_length + 1)\n",
    "    # Calculate the mean value of the spectrogram to use as the mask value\n",
    "    mask_value = np.mean(augmented_spec)\n",
    "    # Apply the time mask and frequency mask\n",
    "    augmented_spec[freq_start:freq_start + freq_mask_length, time_start:time_start + time_mask_length] = mask_value\n",
    "    return augmented_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(file_path, energy_threshold, output_dir, segment_length=14400, sr=44100):\n",
    "    \"\"\"\n",
    "    Process a single audio file to isolate keystrokes and save the segments.\n",
    "    Parameters:\n",
    "    - file_path: Path to the audio file.\n",
    "    - energy_threshold: Energy threshold for isolating keystrokes.\n",
    "    - output_dir: Directory to save the processed segments.\n",
    "    - segment_length: Length of the segment to extract around each keystroke.\n",
    "    - sr: Sample rate for loading the audio.\n",
    "    Returns:\n",
    "    - sample_paths: List of paths to the saved audio segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=sr)\n",
    "    segments = isolate_keystrokes(audio, sr, threshold=energy_threshold, segment_length=segment_length)\n",
    "    \n",
    "    sample_paths = []\n",
    "    for idx, segment in enumerate(segments):\n",
    "        # Apply time shift for data augmentation\n",
    "        augmented_segment = time_shift(segment)\n",
    "        # Save the raw audio segment (can be used for further processing or as input to a model)\n",
    "        segment_path = os.path.join(output_dir, f\"keystroke_{idx}.wav\")\n",
    "        sf.write(segment_path, augmented_segment, sr)\n",
    "        sample_paths.append(segment_path)\n",
    "        \n",
    "        # Optionally, generate and save the mel-spectrogram image\n",
    "        mel_spec = generate_mel_spectrogram(augmented_segment, sr)\n",
    "        mel_spec_aug = spec_augment(mel_spec)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        librosa.display.specshow(mel_spec_aug, sr=sr, x_axis='time', y_axis='mel', cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        image_path = os.path.join(output_dir, f\"keystroke_{idx}.png\")\n",
    "        plt.savefig(image_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "    return sample_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved 30 keystroke samples in 'keystroke_samples'\n"
     ]
    }
   ],
   "source": [
    "file_path = DATASET_PATH + '/0.wav'\n",
    "energy_threshold = 100\n",
    "output_directory = 'keystroke_samples'\n",
    "    \n",
    "sample_files = process_audio_file(file_path, energy_threshold, output_directory)\n",
    "print(f\"Extracted and saved {len(sample_files)} keystroke samples in '{output_directory}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
