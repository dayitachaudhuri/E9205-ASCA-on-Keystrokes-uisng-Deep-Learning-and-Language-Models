{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from AudioKeystrokeDataset import AudioKeystrokeDataset\n",
    "from CoatNet import CoAtNet\n",
    "from Trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "DATASET_PATH = config['DATASET_PATH']['all']\n",
    "model_id = config['MODELS']['llama3-8B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Dataset fot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioKeystrokeDataset(DATASET_PATH, full_dataset=True)\n",
    "print(f\"Dataset contains {len(dataset)} keystroke samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/label2idx.json\", \"w\") as f:\n",
    "    json.dump(dataset.label2idx, f)\n",
    "with open(\"data/device2idx.json\", \"w\") as f:\n",
    "    json.dump(dataset.device2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get labels per sample\n",
    "all_labels = [dataset.label2idx[label] for _, label, _ in dataset.samples] \n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Step 2: First stratified split (train vs val+test)\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, valtest_idx in sss1.split(np.zeros(len(all_labels)), all_labels):\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    valtest_labels = all_labels[valtest_idx]\n",
    "    valtest_dataset = Subset(dataset, valtest_idx)\n",
    "\n",
    "# Step 3: Second stratified split (val vs test)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for val_idx, test_idx in sss2.split(np.zeros(len(valtest_labels)), valtest_labels):\n",
    "    val_dataset = Subset(valtest_dataset, val_idx)\n",
    "    test_dataset = Subset(valtest_dataset, test_idx)\n",
    "\n",
    "# Step 4: Print final sizes\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32,num_workers=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoAtNet(num_classes=len(dataset.label2idx), num_devices=len(dataset.device2idx), in_channels=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device, scheduler, early_stopping_patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train(num_epochs=200, save_path='models/model_all.pth', best_save_path='models/best_model_all.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        if len(data.shape) == 3:\n",
    "            data = data.unsqueeze(1)\n",
    "        outputs = model(data)\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store true labels and predictions\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        # Ensure correct input dimensions\n",
    "        if len(data.shape) == 3:\n",
    "            data = data.unsqueeze(1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Store results\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_targets = np.array(all_targets)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# Get unique class labels\n",
    "class_labels = np.unique(all_targets)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_predictions, labels=class_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=False, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "# Remove x and y axis ticks completely\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_MAPPINGS = {\n",
    "    '.': ['fullstop'],\n",
    "    ',': ['comma(,)'],\n",
    "    \"'\": [\"apostrophe(')\"],\n",
    "    '/': ['slash'],\n",
    "    '\\\\': ['backslash'],\n",
    "    '?': ['Lshift', 'slash'],\n",
    "    ';': ['semicolon(;)'],\n",
    "    ':': ['Lshift', 'semicolon(;)'],\n",
    "    '-' : ['dash(-)'],\n",
    "    '_': ['Lshift', 'dash(-)'],\n",
    "    '=': ['equal(=)'],\n",
    "    '+': ['Lshift', 'equal(=)'],\n",
    "    ')': ['Lshift', '0'],\n",
    "    '!': ['Lshift', '1'],\n",
    "    '@': ['Lshift', '2'],\n",
    "    '#': ['Lshift', '3'],\n",
    "    '$': ['Lshift', '4'],\n",
    "    '%': ['Lshift', '5'],\n",
    "    '^': ['Lshift', '6'],\n",
    "    '&': ['Lshift', '7'],\n",
    "    '*': ['Lshift', '8'],\n",
    "    '(': ['Lshift', '9'],\n",
    "    '{': ['Lshift', 'bracketopen([)'],\n",
    "    '}': ['Lshift', 'bracketclose(])'],\n",
    "    '[': ['bracketopen([)'],\n",
    "    ']': ['bracketclose(])'],\n",
    "    ' ': ['space'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keystrokes_to_text(predicted_keys):\n",
    "    reverse_map = {tuple(v): k for k, v in CHAR_MAPPINGS.items()}\n",
    "    result = []\n",
    "    i = 0\n",
    "    caps_mode = False\n",
    "    while i < len(predicted_keys):\n",
    "        key = predicted_keys[i]\n",
    "        if key == 'caps':\n",
    "            caps_mode = not caps_mode\n",
    "            i += 1\n",
    "            continue\n",
    "        matched = False\n",
    "        for kseq, char in reverse_map.items():\n",
    "            if predicted_keys[i:i + len(kseq)] == list(kseq):\n",
    "                result.append(char)\n",
    "                i += len(kseq)\n",
    "                matched = True\n",
    "                break\n",
    "        if matched:\n",
    "            continue\n",
    "        if key == 'space':\n",
    "            result.append(' ')\n",
    "        # Handle regular characters\n",
    "        elif len(key) == 1:\n",
    "            result.append(key.upper() if caps_mode else key)\n",
    "        # Unrecognized/special key\n",
    "        else:\n",
    "            result.append(f\"<{key}>\")\n",
    "        i += 1\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from save_individual_keystrokes import isolate_keystrokes, generate_mel_spectrogram\n",
    "\n",
    "# === Paths ===\n",
    "SENT_DIR = \"data/sentences\"\n",
    "LABEL_PATH = \"data/label2idx.json\"\n",
    "DEVICE_MAP_PATH = os.path.join(SENT_DIR, \"sentence2device.json\")\n",
    "\n",
    "# === Constants ===\n",
    "SR = 44100\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH_STFT = 256\n",
    "SEGMENT_LENGTH = 14400\n",
    "N_MELS = 64\n",
    "SPEC_HOP_LENGTH = 500\n",
    "THRESHOLD = 100  # Use if isolate_keystrokes needs it\n",
    "\n",
    "# === Load Label Mappings ===\n",
    "with open(LABEL_PATH, 'r') as f:\n",
    "    label2idx = json.load(f)\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "# === Load Sentence to Device Map ===\n",
    "with open(DEVICE_MAP_PATH, 'r') as f:\n",
    "    sentence2device = json.load(f)\n",
    "\n",
    "# === Model Eval Mode ===\n",
    "model.eval()\n",
    "\n",
    "# === Collect Predictions ===\n",
    "predictions = {}\n",
    "\n",
    "for fname in tqdm(os.listdir(SENT_DIR)):\n",
    "    if not fname.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    sid = fname.split(\".\")[0]\n",
    "    wav_path = os.path.join(SENT_DIR, fname)\n",
    "\n",
    "    if sid not in sentence2device:\n",
    "        print(f\"Skipping {sid}: device not found\")\n",
    "        continue\n",
    "\n",
    "    device_name = sentence2device[sid]\n",
    "    device_id = torch.tensor([dataset.device2idx[device_name]]).to(device)\n",
    "\n",
    "    audio, _ = librosa.load(wav_path, sr=SR)\n",
    "    threshold, segments, segment_starts = isolate_keystrokes(\n",
    "        audio,\n",
    "        sr=SR,\n",
    "        segment_length=SEGMENT_LENGTH,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH_STFT\n",
    "    )\n",
    "\n",
    "    if not segments:\n",
    "        print(f\"No keystrokes detected in {fname}\")\n",
    "        predictions[sid] = \"\"\n",
    "        continue\n",
    "\n",
    "    pred_chars = []\n",
    "    with torch.no_grad():\n",
    "        for seg in segments:\n",
    "            mel = generate_mel_spectrogram(seg, sr=SR)\n",
    "            mel = (mel - mel.min()) / (mel.max() - mel.min() + 1e-6)\n",
    "            x = torch.from_numpy(mel).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            out = model(x, device_id)\n",
    "            probs = torch.nn.functional.softmax(out, dim=1)\n",
    "            idx = probs.argmax(dim=1).item()\n",
    "            pred_chars.append(idx2label[idx])\n",
    "\n",
    "    text = keystrokes_to_text(pred_chars)\n",
    "    predictions[sid] = text\n",
    "\n",
    "# === Final Output ===\n",
    "print(\"\\nPredictions:\")\n",
    "for sid, text in predictions.items():\n",
    "    print(f\"{sid}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''You are a helpful assistant that corrects typos in sentences based on the likely intent of the user. Each typo was generated by a model that tries to guess typed words based on keystroke sounds. \n",
    "\n",
    "The correction should be the closest valid sentence with proper spelling and grammar, assuming the model made as few mistakes as possible.\n",
    "\n",
    "Examples are given below.\n",
    "\n",
    "Typo: \"the wuick bronw fix\"\n",
    "Correct: \"the quick brown fox\"\n",
    "\n",
    "Typo: \"how arw ypu\"\n",
    "Correct: \"how are you\"\n",
    "\n",
    "Typo: \"in tghe beeginning\"\n",
    "Correct: \"in the beginning\"\n",
    "\n",
    "Typo: \"whi is thsi hapenung\"\n",
    "Correct: \"why is this happening\"\n",
    "\n",
    "Typo: \"i kniw waht im doinf\"\n",
    "Correct: \"i know what I'm doing\"\n",
    "\n",
    "Typo: \"plrase snd help\"\n",
    "Correct: \"please send help\"\n",
    "\n",
    "Correct each of the following typo sentences. Give the corrected sentence only, without any additional text or explanation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Combine all messages into a single prompt string\n",
    "prompt = PROMPT\n",
    "for sentence in list(predictions.values())[:1]:\n",
    "    prompt += f'\\nTypo: \"{sentence}\"'\n",
    "\n",
    "# Now generate using the text-generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "outputs = generator(prompt, max_new_tokens=256)\n",
    "generated_text = outputs[0]['generated_text']\n",
    "\n",
    "# Print the full output\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    messages = output['generated_text']\n",
    "    assistant_response = next(\n",
    "        (msg['content'] for msg in reversed(messages) if msg['role'] == 'assistant'),\n",
    "        None\n",
    "    )\n",
    "    print(assistant_response)\n",
    "    print(\"===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
